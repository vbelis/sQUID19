{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small experiment we try to use hybrid-classical approach in order to represent Quadratic unconstarait binari optimization as problem of finding minimum of Hamiltonian of Ising model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical realization\n",
    "Used a classical ensemble of ML estimators  such as Random Foster, Boosting etc. \n",
    "\n",
    "The loss function of our ML algorithm has the form:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}w_kh_k(x_i)-\n",
    "y_i\\right)^2+\\lambda\\|w\\|_0\\right),\n",
    "$$\n",
    "\n",
    "where $h_k(x_i)$ is the prediction of the weak estimator $k$ and $x_i$ are the data points. The weights $w_k$ are binary. The regularization in the $l_0$ norm ensures sparsity.\n",
    "\n",
    "Expand the quadratic part:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\left( \\left(\\sum_{k=1}^{K} w_k h_k(x_i)\\right)^{2} -\n",
    "2\\sum_{k=1}^{K} w_k h_k(\\mathbf{x}_i)y_i + y_i^{2}\\right) + \\lambda \\|w\\|_{0}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Since $y_i^{2}$ is just a constant offset, the optimization reduces to\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\n",
    "\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{l=1}^{K} w_k w_l\n",
    "\\left(\\sum_{i=1}^{N}h_k(x_i)h_l(x_i)\\right) - \n",
    "\\frac{2}{N}\\sum_{k=1}^{K}w_k\\sum_{i=1}^{N} h_k(x_i)y_i +\n",
    "\\lambda \\|w\\|_{0} \\right).\n",
    "$$\n",
    "\n",
    "- $h_k(x_i)h_l(x_i)$: correlations between the predictions of the weak learners.\n",
    "\n",
    "- $h_k(x_i)y_i$: correlation with the true label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping loss function of an ensemble of models to the 1D Ising model Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost (loss) function of ML algorithms with binary weights can be written in a quadratic form:\n",
    "$$\n",
    "F(x)=\\sum_{i,j} x_iA_{ij}x_j,\\quad x\\in \\{0,1\\}^n\n",
    "$$\n",
    "where x is a n-vector of binary variables, and $A$ is a $n\\times n$ square symmetric matrix.\n",
    "\n",
    "\n",
    "A loss function of this form can be mapped to the the Ising model Hamiltonian. Thereby the minimization of $F$ becomes a search of the ground state (i.e. minimum energy-eigenvalue). \n",
    "\n",
    "$$\n",
    " \\hat{H}=-\\sum_{<i,j>} J_{ij} \\sigma^Z_i \\sigma^Z_{j} - \\sum_i h_i \\sigma^Z_i\n",
    "$$\n",
    "\n",
    "\n",
    "Also the *traverse 1D Ising model* can be used\n",
    "\n",
    "$$\n",
    " \\hat{H}=-\\sum_{<i,j>} J_{ij} \\sigma^Z_i \\sigma^Z_{j} - \\sum_i h_i \\sigma^Z_i - \\sum_i g_i \\sigma^X_i\n",
    "$$\n",
    "\n",
    "The transverse field is important for the quantum approximation optimization algorithms. The $\\sigma^X_i$ does not commute with the other terms. It becomes possible to exploit quantum effects like tunnelling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimization of the loss function using QAOA (Qiskit)\n",
    "- Gets input of the free parameters of the $F$ and creates a Ising Hamiltonian from them\n",
    "- The quantum alogirthm produces a ansatz (which depends on the specific algorithm implemented) that depends on $\\theta_j$ parameters $|i(\\theta_j)\\rangle$\n",
    "- Acts on the trial state with unitary transformations $M$ times.\n",
    "$$\n",
    "|\\Psi(\\theta_j)\\rangle = \\sum_{k=1}^{M} U_k(\\theta_k) |i(\\theta_j)\\rangle\n",
    "$$\n",
    "- Calculates the expectation values $\\langle|\\Psi(\\theta)|\\hat{H}|\\Psi(\\theta)\\rangle$\n",
    "- Iterate and exploit quantum tunneling to find the eigenstate with the smallest energy\n",
    "- Outputs the parameters which correspond to the ground state of $\\hat{H}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go back to classical loss function and evaluate inference of the optimized model\n",
    "Get the parameters which minimize $F$ and test on data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "- Data-set small ($\\sim 500$ train)\n",
    "- Small number of estimators (ensemble of 3 estimator). Stronger machine and longer training time is needed for more estimators, in order to have better inference.\n",
    "- We transform text samples  to frequency of words which is sparse and have many zero elements. This can lead to some ty pe of numerical internal calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
